{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import hypertune\n",
    "import munch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "MODEL_FILE_NAME = 'torch.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "              100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "              output, target, size_average=False).item()  # sum up batch loss\n",
    "            pred = output.max(\n",
    "              1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          test_loss, correct, len(test_loader.dataset),\n",
    "          100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "        # Uses hypertune to report metrics for hyperparameter tuning.\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='my_loss',\n",
    "          metric_value=test_loss,\n",
    "          global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test():\n",
    "    # Training settings\n",
    "    #args = get_args()\n",
    "    args = munch.Munch(batch_size=64, epochs=10, log_interval=10, lr=0.01, model_dir=None, momentum=0.5, \n",
    "                       no_cuda=False, seed=1, test_batch_size=1000)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(args.seed)\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print(\"Using {} for training.\".format(device))\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "      datasets.MNIST(\n",
    "          '../data',\n",
    "          train=True,\n",
    "          download=True,\n",
    "          transform=transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              # Normalize a tensor image with mean and standard deviation\n",
    "              transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "          ])),\n",
    "      batch_size=args.batch_size,\n",
    "      shuffle=True,\n",
    "      **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "      datasets.MNIST(\n",
    "          '../data',\n",
    "          train=False,\n",
    "          transform=transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              # Normalize a tensor image with mean and standard deviation              \n",
    "              transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "          ])),\n",
    "      batch_size=args.test_batch_size,\n",
    "      shuffle=True,\n",
    "      **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "\n",
    "    if args.model_dir:\n",
    "        tmp_model_file = os.path.join('/tmp', MODEL_FILE_NAME)\n",
    "        torch.save(model.state_dict(), tmp_model_file)\n",
    "        subprocess.check_call([\n",
    "            'gsutil', 'cp', tmp_model_file,\n",
    "            os.path.join(args.model_dir, MODEL_FILE_NAME)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fairing\n",
    "\n",
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job'.format(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = \"gcr.io/caip-dexter-bugbash/gtc-demo:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fairing.config.set_builder('docker', registry=DOCKER_REGISTRY, base_image=base_image)\n",
    "fairing.config.set_deployer('gcp', scale_tier='BASIC_GPU', region='us-east1')\n",
    "remote_train = fairing.config.fn(train_and_test)\n",
    "remote_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
